{-# START_FILE package.yaml #-}
name:                {{name}}
version:             0.1.0.0
github:              "githubuser/{{name}}"
license:             BSD-3-Clause
author:              "Author name here"
maintainer:          "example@example.com"
copyright:           "2026 Author name here"

extra-source-files:
- README.md
- CHANGELOG.md

description:         Please see the README on GitHub at <https://github.com/githubuser/{{name}}#readme>

dependencies:
- base >= 4.7 && < 5
- rio
- unliftio
- aeson
- text
- servant
- servant-server
- warp
- wai
- persistent
- persistent-sqlite
- persistent-postgresql
- esqueleto
- monad-logger
- hw-kafka-client
- containers
- envy
- vault
- random
- fast-logger
- resource-pool
- haskell-service-lib
- uuid
- time
- lens
- microlens
- mtl
- bytestring
- unordered-containers
- http-types
- http-client

ghc-options:
- -Wall
- -Wcompat
- -Widentities
- -Wincomplete-record-updates
- -Wincomplete-uni-patterns
- -Wmissing-export-lists
- -Wmissing-home-modules
- -Wpartial-fields
- -Wredundant-constraints

default-extensions:
  - LambdaCase
  - OverloadedRecordDot
  - OverloadedStrings
  - NoImplicitPrelude
  - BangPatterns
  - BinaryLiterals
  - DeriveGeneric
  - ScopedTypeVariables
  - DataKinds
  - TypeOperators
  - RecordWildCards
  - FunctionalDependencies
  - AllowAmbiguousTypes
  - TypeApplications
  - FlexibleInstances
  - UndecidableInstances
  - DerivingStrategies
  - DeriveAnyClass

library:
  source-dirs: src

executables:
  {{name}}-exe:
    main:                Main.hs
    source-dirs:         app
    ghc-options:
    - -threaded
    - -rtsopts
    - -with-rtsopts=-N
    dependencies:
    - {{name}}

  {{name}}-migrations:
    main:                Main.hs
    source-dirs:         migrations-app
    ghc-options:
    - -threaded
    - -rtsopts
    - -with-rtsopts=-N
    dependencies:
    - {{name}}

tests:
  {{name}}-test:
    main:                Spec.hs
    source-dirs:         test
    ghc-options:
    - -threaded
    - -rtsopts
    - -with-rtsopts=-N
    dependencies:
    - {{name}}
    - hspec
    - http-client
    - http-types
    - wai
    - warp

{-# START_FILE README.md #-}
# {{name}}

Generated from the monorepo service template.

## Quickstart

### 1. Add to the monorepo

```
# In stack.yaml, add under packages:
- services/{{name}}
```

### 2. Rename the placeholder domain

Search-replace `Item` / `item` with your entity name in:
- `src/Models/Item.hs`
- `src/Domain/Items.hs`
- `src/Ports/Repository.hs`
- `src/Ports/Produce.hs`
- `src/Ports/Server.hs`
- `test/Spec.hs`

### 3. Build and run

```sh
stack build {{name}}
stack run {{name}}-exe
stack run {{name}}-migrations   # run DB migrations standalone
stack test {{name}}
```

## Configuration

| Variable | Default | Description |
|---|---|---|
| `PORT` | `8080` | HTTP server port |
| `ENVIRONMENT` | `development` | Runtime environment |
| `DB_TYPE` | `postgresql` | `postgresql` or `sqlite` |
| `DB_CONNECTION_STRING` | — | Database URL |
| `DB_POOL_SIZE` | `10` | Connection pool size |
| `DB_AUTO_MIGRATE` | `false` | Run migrations on startup |
| `KAFKA_BROKER` | `localhost:9092` | Kafka broker |
| `KAFKA_GROUP_ID` | `{{name}}` | Consumer group ID |
| `KAFKA_DEAD_LETTER_TOPIC` | `DEADLETTER` | DLQ topic name |
| `KAFKA_MAX_RETRIES` | `3` | Max retries before DLQ |

## Architecture

```
src/
  App.hs              RIO environment, instance declarations
  Lib.hs              Startup orchestration
  Settings.hs         Environment variable loading
  Domain/
    Items.hs          Business logic (rename to your entity)
  Models/
    Item.hs           Persistent schema (rename to your entity)
  Ports/
    Server.hs         Servant HTTP adapter (thin — delegates to Domain)
    Consumer.hs       Kafka consumer config + handlers
    Produce.hs        Kafka publishing helpers
    Repository.hs     Database queries
```

{-# START_FILE CHANGELOG.md #-}
# Changelog

## Unreleased

{-# START_FILE app/Main.hs #-}
module Main (main) where

import Lib
import RIO

main :: IO ()
main = app

{-# START_FILE migrations-app/Main.hs #-}
-- | Standalone migration runner for production deployments.
-- Usage: stack exec {{name}}-migrations
module Main (main) where

import Control.Monad.Logger (runStderrLoggingT)
import Database.Persist.Sql (runMigration, runSqlPool)
import Models.Item (migrateAll)
import RIO
import qualified Service.Database as Database
import System.IO (putStrLn)

main :: IO ()
main = do
  putStrLn "=== {{name}} Migration Runner ==="
  putStrLn "WARNING: Only run this on a single instance!"
  putStrLn ""

  logOptions <- logOptionsHandle stderr False
  withLogFunc logOptions $ \logFunc -> runRIO logFunc $ do
    dbSettings <- Database.decoder

    logInfo $ "Database type: " <> displayShow (Database.dbType dbSettings)
    logInfo "Connecting to database..."

    pool <- liftIO $ Database.createConnectionPool dbSettings

    logInfo "Running migrations..."

    liftIO $ runStderrLoggingT $ runSqlPool (runMigration migrateAll) pool

    logInfo "Migrations completed successfully!"

{-# START_FILE src/Lib.hs #-}
module Lib
  ( app,
  )
where

import qualified App
import RIO
import Settings (loadSettings)

app :: IO ()
app = do
  logOptions <- logOptionsHandle stderr True
  withLogFunc logOptions $ \logFunc -> do
    settings <- loadSettings logFunc
    appEnv <- App.initializeApp settings logFunc
    App.runApp appEnv

{-# START_FILE src/App.hs #-}
module App
  ( App (..),
    HasKafkaProducerHandle (..),
    HasLogContext (..),
    initializeApp,
    runApp,
    app,
  )
where

import Control.Monad.Logger (runStderrLoggingT)
import qualified Data.Map.Strict as Map
import Database.Persist.Sql (ConnectionPool, runMigration, runSqlPool)
import Kafka.Producer (KafkaProducer)
import Models.Item (migrateAll)
import Network.Wai.Handler.Warp (run)
import qualified Ports.Consumer as KafkaPort
import qualified Ports.Server as Server
import RIO
import Servant (hoistServer, serve)
import Service.CorrelationId
  ( CorrelationId (..),
    HasCorrelationId (..),
    HasLogContext (..),
    correlationIdMiddleware,
    defaultCorrelationId,
    extractCorrelationId,
    logInfoC,
    unCorrelationId,
  )
import Service.Database (HasDB (..))
import Service.Metrics.Database (recordDatabaseMetricsInternal)
import qualified Service.Database as Database
import Service.HttpClient (HasHttpClient (..), HttpClient)
import qualified Service.HttpClient as HttpClient
import Service.Kafka (HasKafkaProducer (..))
import qualified Service.Kafka as Kafka
import Service.Metrics (HasMetrics (..), Metrics, initMetrics)
import Settings (Settings (..), server)

data App = App
  { appLogFunc :: !LogFunc,
    appLogContext :: !(Map Text Text),
    appSettings :: !Settings,
    appCorrelationId :: !CorrelationId,
    db :: !ConnectionPool,
    kafkaProducer :: !KafkaProducer,
    httpClient :: !HttpClient,
    appMetrics :: !Metrics
  }

instance HasLogFunc App where
  logFuncL = lens appLogFunc (\x y -> x {appLogFunc = y})

instance HasLogContext App where
  logContextL = lens appLogContext (\x y -> x {appLogContext = y})

instance HasCorrelationId App where
  correlationIdL = lens appCorrelationId (\x y -> x {appCorrelationId = y})

instance Server.HasConfig App Settings where
  settingsL = lens appSettings (\x y -> x {appSettings = y})
  httpSettings = server

instance HasDB App where
  dbL = lens db (\x y -> x {db = y})
  dbRecordQueryMetrics = recordDatabaseMetricsInternal

class HasKafkaProducerHandle env where
  kafkaProducerL :: Lens' env KafkaProducer

instance HasKafkaProducerHandle App where
  kafkaProducerL = lens kafkaProducer (\x y -> x {kafkaProducer = y})

instance HasKafkaProducer App where
  produceKafkaMessage topic key value = do
    producer <- view kafkaProducerL
    cid <- view correlationIdL
    Kafka.produceMessageWithCid producer topic key value cid

instance HasHttpClient App where
  httpClientL = lens httpClient (\x y -> x {httpClient = y})

instance HasMetrics App where
  metricsL = lens appMetrics (\x y -> x {appMetrics = y})

initializeApp :: Settings -> LogFunc -> IO App
initializeApp settings logFunc = runRIO logFunc $ do
  let dbSettings = database settings
      kafkaSettings = kafka settings

  pool <- liftIO $ Database.createConnectionPool dbSettings

  when (Database.dbAutoMigrate dbSettings) $ do
    logInfo "Running database migrations (DB_AUTO_MIGRATE=true)"
    liftIO $ runStderrLoggingT $ runSqlPool (runMigration migrateAll) pool

  producer <- Kafka.startProducer (KafkaPort.kafkaBroker kafkaSettings)
  client <- HttpClient.initHttpClient
  metrics <- liftIO initMetrics

  let initCid = defaultCorrelationId
      initContext = Map.singleton "cid" (unCorrelationId initCid)

  return
    App
      { appLogFunc = logFunc,
        appLogContext = initContext,
        appSettings = settings,
        appCorrelationId = initCid,
        db = pool,
        kafkaProducer = producer,
        httpClient = client,
        appMetrics = metrics
      }

runApp :: App -> IO ()
runApp env = do
  let settings = appSettings env
      serverSettings = server settings
      kafkaSettings = kafka settings

  runRIO env $ do
    let consumerCfg = KafkaPort.consumerConfig kafkaSettings
    consumer <- Kafka.startConsumer consumerCfg

    race_ (serverThread serverSettings) (kafkaThread consumer consumerCfg)
  where
    serverThread :: Server.Settings -> RIO App ()
    serverThread serverSettings = do
      appEnv <- ask
      logInfoC $ "Starting HTTP server on port " <> displayShow (Server.httpPort serverSettings)
      liftIO $ run (Server.httpPort serverSettings) (app appEnv)

    kafkaThread :: Kafka.KafkaConsumer -> Kafka.ConsumerConfig App -> RIO App ()
    kafkaThread consumer consumerCfg = do
      logInfoC "Starting Kafka consumer"
      Kafka.consumerLoop consumer consumerCfg

app :: App -> Application
app baseEnv = correlationIdMiddleware $ \req ->
  let maybeCid = extractCorrelationId req
      cid = fromMaybe (error "CID middleware should always set CID") maybeCid
      cidText = unCorrelationId cid
      env =
        baseEnv
          & correlationIdL .~ cid
          & logContextL .~ Map.singleton "cid" cidText
   in serve (Proxy :: Proxy Server.API) (hoistServer (Proxy :: Proxy Server.API) (runRIO env) Server.server) req

{-# START_FILE src/Settings.hs #-}
module Settings
  ( Settings (..),
    loadSettings,
  )
where

import qualified Ports.Consumer as KafkaPort
import qualified Ports.Server as Server
import RIO
import qualified Service.Database as Database

data Settings = Settings
  { server :: !Server.Settings,
    kafka :: !KafkaPort.Settings,
    database :: !Database.Settings
  }

decoder :: (HasLogFunc env) => RIO env Settings
decoder = do
  serverSettings <- Server.decoder
  kafkaSettings <- KafkaPort.decoder
  dbSettings <- Database.decoder
  return
    Settings
      { server = serverSettings,
        kafka = kafkaSettings,
        database = dbSettings
      }

loadSettings :: LogFunc -> IO Settings
loadSettings logFunc = runRIO logFunc decoder

{-# START_FILE src/Models/Item.hs #-}
{-# LANGUAGE DeriveAnyClass #-}
{-# LANGUAGE DerivingStrategies #-}
{-# LANGUAGE FlexibleInstances #-}
{-# LANGUAGE GADTs #-}
{-# LANGUAGE GeneralizedNewtypeDeriving #-}
{-# LANGUAGE MultiParamTypeClasses #-}
{-# LANGUAGE QuasiQuotes #-}
{-# LANGUAGE StandaloneDeriving #-}
{-# LANGUAGE TemplateHaskell #-}
{-# LANGUAGE TypeFamilies #-}
{-# LANGUAGE UndecidableInstances #-}

module Models.Item (Item (..), migrateAll, ItemId) where

import Data.Aeson (FromJSON, ToJSON)
import Database.Persist.TH
import RIO

share
  [mkPersist sqlSettings, mkMigrate "migrateAll"]
  [persistLowerCase|
Item
  name Text
  description Text Maybe
  deriving Show Generic ToJSON FromJSON
  |]

{-# START_FILE src/Domain/Items.hs #-}
module Domain.Items
  ( Domain,
    listItems,
    getItemById,
    createItem,
  )
where

import Database.Persist.Sql (entityVal, fromSqlKey)
import Models.Item (Item (..))
import Ports.Produce (publishItemCreated)
import qualified Ports.Repository as Repo
import RIO
import Servant (err404, errBody)
import Service.CorrelationId (HasLogContext (..), logInfoC)
import Service.Database (HasDB (..))
import Service.Kafka (HasKafkaProducer (..))

-- | Constraint alias for domain functions that need DB access.
type Domain env = (HasLogFunc env, HasLogContext env, HasDB env)

-- | List all items.
listItems :: Domain env => RIO env [Item]
listItems = do
  logInfoC "Listing items"
  entities <- Repo.findAllItems
  return $ map entityVal entities

-- | Fetch a single item by ID; throws 404 if not found.
getItemById :: Domain env => Int64 -> RIO env Item
getItemById itemId = do
  logInfoC $ "Getting item: " <> displayShow itemId
  mItem <- Repo.findItemById itemId
  case mItem of
    Nothing -> throwM err404 {errBody = "Item not found"}
    Just item -> return item

-- | Create a new item and publish an event.
createItem :: (Domain env, HasKafkaProducer env) => Text -> Maybe Text -> RIO env Item
createItem name desc = do
  logInfoC $ "Creating item: " <> display name
  let newItem = Item {itemName = name, itemDescription = desc}
  itemId <- Repo.createItem newItem
  publishItemCreated (fromSqlKey itemId) name
  return newItem

{-# START_FILE src/Ports/Server.hs #-}
module Ports.Server
  ( API,
    Routes (..),
    CreateItemRequest (..),
    HasConfig (..),
    server,
    module Service.Server,
  )
where

import Data.Aeson (FromJSON, ToJSON)
import Domain.Items (Domain)
import qualified Domain.Items as Domain
import Models.Item (Item)
import RIO
import Servant
import Servant.Server.Generic (AsServerT)
import Service.CorrelationId (HasLogContext (..))
import Service.Kafka (HasKafkaProducer (..))
import Service.Metrics (HasMetrics (..), metricsHandler)
import Service.Server

-- ============================================================================
-- Request / Response types
-- ============================================================================

data CreateItemRequest = CreateItemRequest
  { createItemName :: !Text,
    createItemDescription :: !(Maybe Text)
  }
  deriving stock (Show, Eq, Generic)
  deriving anyclass (FromJSON, ToJSON)

-- ============================================================================
-- Routes
-- ============================================================================

data Routes route = Routes
  { status ::
      route
        :- Summary "Health check endpoint"
          :> "status"
          :> Get '[JSON] Text,
    getItems ::
      route
        :- Summary "List all items"
          :> "items"
          :> Get '[JSON] [Item],
    getItem ::
      route
        :- Summary "Get item by ID"
          :> "items"
          :> Capture "id" Int64
          :> Get '[JSON] Item,
    createItem ::
      route
        :- Summary "Create a new item"
          :> "items"
          :> ReqBody '[JSON] CreateItemRequest
          :> Post '[JSON] Item,
    getMetrics ::
      route
        :- Summary "Prometheus metrics endpoint"
          :> "metrics"
          :> Get '[PlainText] Text
  }
  deriving stock (Generic)

type API = NamedRoutes Routes

-- ============================================================================
-- HasConfig
-- ============================================================================

class HasConfig env settings | env -> settings where
  settingsL :: Lens' env settings
  httpSettings :: settings -> Settings

-- ============================================================================
-- Server (thin adapter — delegates to Domain)
-- ============================================================================

server ::
  ( HasLogFunc env,
    HasLogContext env,
    HasConfig env settings,
    HasDB env,
    HasKafkaProducer env,
    HasMetrics env
  ) =>
  Routes (AsServerT (RIO env))
server =
  Routes
    { status = statusHandler,
      getItems = Domain.listItems,
      getItem = Domain.getItemById,
      createItem = \req -> Domain.createItem (createItemName req) (createItemDescription req),
      getMetrics = metricsEndpointHandler
    }

statusHandler ::
  forall env settings.
  (HasLogFunc env, HasLogContext env, HasConfig env settings) =>
  RIO env Text
statusHandler = do
  settings <- view (settingsL @env @settings)
  let serverSettings = httpSettings @env @settings settings
  logInfoC ("Status OK, env=" <> displayShow (httpEnvironment serverSettings))
  return "OK"

metricsEndpointHandler :: (HasMetrics env) => RIO env Text
metricsEndpointHandler = do
  metrics <- view metricsL
  liftIO $ metricsHandler metrics

{-# START_FILE src/Ports/Consumer.hs #-}
module Ports.Consumer
  ( module Service.Kafka,
    consumerConfig,
  )
where

import Data.Aeson (Value)
import Kafka.Consumer (TopicName (..))
import RIO
import Service.CorrelationId (HasLogContext (..), logInfoC)
import Service.Kafka

-- | Build the Kafka consumer config.
-- Add topic handlers below; each handler delegates to Domain functions.
consumerConfig :: Settings -> ConsumerConfig env
consumerConfig kafkaSettings =
  ConsumerConfig
    { brokerAddress = kafkaBroker kafkaSettings,
      groupId = kafkaGroupId kafkaSettings,
      topicHandlers =
        [ TopicHandler
            { topic = TopicName "item-created",
              handler = itemCreatedHandler
            }
        ],
      deadLetterTopic = TopicName (kafkaDeadLetterTopic kafkaSettings),
      maxRetries = kafkaMaxRetries kafkaSettings,
      consumerRecordMessageMetrics = \_ _ _ _ -> return (),
      consumerRecordOffsetMetrics = \_ _ _ _ -> return ()
    }

-- | Example consumer handler. Replace with real Domain calls.
itemCreatedHandler :: (HasLogFunc env, HasLogContext env) => Value -> RIO env ()
itemCreatedHandler jsonValue =
  logInfoC $ "Received item-created event: " <> displayShow jsonValue

{-# START_FILE src/Ports/Produce.hs #-}
module Ports.Produce
  ( ItemCreatedEvent (..),
    publishItemCreated,
  )
where

import Data.Aeson (ToJSON)
import Kafka.Consumer (TopicName (..))
import RIO
import Service.Kafka (HasKafkaProducer (..))

data ItemCreatedEvent = ItemCreatedEvent
  { itemCreatedId :: !Int64,
    itemCreatedName :: !Text
  }
  deriving stock (Show, Generic)
  deriving anyclass (ToJSON)

publishItemCreated :: HasKafkaProducer env => Int64 -> Text -> RIO env ()
publishItemCreated itemId name =
  produceKafkaMessage
    (TopicName "item-created")
    Nothing
    ItemCreatedEvent {itemCreatedId = itemId, itemCreatedName = name}

{-# START_FILE src/Ports/Repository.hs #-}
{-# LANGUAGE ConstraintKinds #-}

module Ports.Repository
  ( Repo,
    findAllItems,
    findItemById,
    createItem,
  )
where

import Database.Persist.Sql (Entity, SelectOpt (..), get, insert, selectList, toSqlKey)
import Models.Item (Item, ItemId)
import RIO
import Service.CorrelationId (HasLogContext (..))
import Service.Database (HasDB (..), runSqlPoolWithCid)

type Repo env = (HasLogFunc env, HasLogContext env, HasDB env)

findAllItems :: Repo env => RIO env [Entity Item]
findAllItems = do
  pool <- view dbL
  runSqlPoolWithCid (selectList [] [LimitTo 100]) pool

findItemById :: Repo env => Int64 -> RIO env (Maybe Item)
findItemById itemId = do
  pool <- view dbL
  runSqlPoolWithCid (get (toSqlKey itemId :: ItemId)) pool

createItem :: Repo env => Item -> RIO env ItemId
createItem item = do
  pool <- view dbL
  runSqlPoolWithCid (insert item) pool

{-# START_FILE test/Spec.hs #-}
{-# LANGUAGE FlexibleContexts #-}
{-# LANGUAGE FlexibleInstances #-}

import Control.Monad.Except (ExceptT (..))
import Control.Monad.Logger (runStderrLoggingT)
import Data.Aeson (decode, encode)
import qualified Data.ByteString.Lazy as BSL
import qualified Data.Map.Strict as Map
import Database.Persist.Sql (ConnectionPool, runMigration, runSqlPool)
import Models.Item (Item (..), migrateAll)
import Network.HTTP.Client
  ( Manager,
    RequestBody (..),
    Response,
    defaultManagerSettings,
    httpLbs,
    method,
    newManager,
    parseRequest,
    requestBody,
    requestHeaders,
    responseBody,
    responseStatus,
  )
import Network.HTTP.Types.Status (status200, statusCode)
import Network.Wai (Application)
import Network.Wai.Handler.Warp (testWithApplication)
import qualified Ports.Consumer as KafkaPort
import Ports.Server (API, CreateItemRequest (..), HasConfig (..))
import qualified Ports.Server as Server
import RIO hiding (Handler)
import Servant (Proxy (..))
import Servant.Server (Handler (..), ServerError, hoistServer, serve)
import Service.CorrelationId
  ( CorrelationId (..),
    HasCorrelationId (..),
    HasLogContext (..),
    defaultCorrelationId,
  )
import Service.Database (HasDB (..))
import qualified Service.Database as Database
import Service.Kafka (HasKafkaProducer (..))
import Service.Metrics (HasMetrics (..), Metrics, initMetrics)
import Settings (Settings (..))
import Test.Hspec

-- ============================================================================
-- Test Environment
-- ============================================================================

data TestApp = TestApp
  { testLogFunc :: !LogFunc,
    testLogContext :: !(Map Text Text),
    testSettings :: !Settings,
    testCorrelationId :: !CorrelationId,
    testDb :: !ConnectionPool,
    testMetrics :: !Metrics
  }

instance HasLogFunc TestApp where
  logFuncL = lens testLogFunc (\x y -> x {testLogFunc = y})

instance HasLogContext TestApp where
  logContextL = lens testLogContext (\x y -> x {testLogContext = y})

instance HasCorrelationId TestApp where
  correlationIdL = lens testCorrelationId (\x y -> x {testCorrelationId = y})

instance HasConfig TestApp Settings where
  settingsL = lens testSettings (\x y -> x {testSettings = y})
  httpSettings = server

instance HasDB TestApp where
  dbL = lens testDb (\x y -> x {testDb = y})

instance HasKafkaProducer TestApp where
  produceKafkaMessage _ _ _ = return ()

instance HasMetrics TestApp where
  metricsL = lens testMetrics (\x y -> x {testMetrics = y})

-- ============================================================================
-- Fixtures
-- ============================================================================

withTestApp :: (Int -> IO ()) -> IO ()
withTestApp action = do
  let dbSettings =
        Database.Settings
          { Database.dbType = Database.SQLite,
            Database.dbConnectionString = ":memory:",
            Database.dbPoolSize = 1,
            Database.dbAutoMigrate = False
          }
      testSettings =
        Settings
          { server = Server.Settings {Server.httpPort = 0, Server.httpEnvironment = "test"},
            kafka =
              KafkaPort.Settings
                { KafkaPort.kafkaBroker = "localhost:9092",
                  KafkaPort.kafkaGroupId = "{{name}}-test",
                  KafkaPort.kafkaDeadLetterTopic = "DEADLETTER",
                  KafkaPort.kafkaMaxRetries = 3
                },
            database = dbSettings
          }
  logOptions <- logOptionsHandle stderr False
  withLogFunc logOptions $ \logFunc -> do
    pool <- Database.createConnectionPool dbSettings
    runStderrLoggingT $ runSqlPool (runMigration migrateAll) pool
    metrics <- initMetrics
    let testApp =
          TestApp
            { testLogFunc = logFunc,
              testLogContext = Map.empty,
              testSettings = testSettings,
              testCorrelationId = defaultCorrelationId,
              testDb = pool,
              testMetrics = metrics
            }
    testWithApplication (pure $ appToWai testApp) action

appToWai :: TestApp -> Application
appToWai env =
  serve (Proxy @API) $
    hoistServer (Proxy @API) toHandler Server.server
  where
    -- throwM in RIO throws ServerError as an IO exception.
    -- Handler = ExceptT ServerError IO, so we catch and re-route it.
    toHandler :: forall a. RIO TestApp a -> Handler a
    toHandler action =
      Handler $ ExceptT $
        (Right <$> runRIO env action)
          `catch` (\(e :: ServerError) -> return (Left e))

-- ============================================================================
-- HTTP Helpers
-- ============================================================================

postJSON :: Manager -> String -> BSL.ByteString -> IO (Response BSL.ByteString)
postJSON mgr url body = do
  req <- parseRequest url
  httpLbs
    req
      { method = "POST",
        requestBody = RequestBodyLBS body,
        requestHeaders = [("Content-Type", "application/json")]
      }
    mgr

baseUrl :: Int -> String
baseUrl port = "http://localhost:" <> show port

-- ============================================================================
-- Tests
-- ============================================================================

spec :: Spec
spec = around withTestApp $ do
  describe "GET /status" $ do
    it "returns 200 OK" $ \port -> do
      mgr <- newManager defaultManagerSettings
      req <- parseRequest (baseUrl port <> "/status")
      resp <- httpLbs req mgr
      responseStatus resp `shouldBe` status200

  describe "GET /items" $ do
    it "returns an empty list initially" $ \port -> do
      mgr <- newManager defaultManagerSettings
      req <- parseRequest (baseUrl port <> "/items")
      resp <- httpLbs req mgr
      responseStatus resp `shouldBe` status200
      decode @[Item] (responseBody resp) `shouldBe` Just []

  describe "POST /items" $ do
    it "creates an item and returns it" $ \port -> do
      mgr <- newManager defaultManagerSettings
      let createReq = CreateItemRequest {createItemName = "Widget", createItemDescription = Just "A test widget"}
      resp <- postJSON mgr (baseUrl port <> "/items") (encode createReq)
      responseStatus resp `shouldBe` status200
      let mItem = decode @Item (responseBody resp)
      mItem `shouldSatisfy` isJust
      case mItem of
        Nothing -> expectationFailure "Failed to decode Item"
        Just item -> do
          itemName item `shouldBe` "Widget"
          itemDescription item `shouldBe` Just "A test widget"

  describe "GET /items/:id" $ do
    it "returns the item after creation" $ \port -> do
      mgr <- newManager defaultManagerSettings
      let createReq = CreateItemRequest {createItemName = "Gadget", createItemDescription = Nothing}
      _ <- postJSON mgr (baseUrl port <> "/items") (encode createReq)
      req <- parseRequest (baseUrl port <> "/items/1")
      resp <- httpLbs req mgr
      responseStatus resp `shouldBe` status200

    it "returns 404 for a missing item" $ \port -> do
      mgr <- newManager defaultManagerSettings
      req <- parseRequest (baseUrl port <> "/items/999")
      resp <- httpLbs req mgr
      statusCode (responseStatus resp) `shouldBe` 404

main :: IO ()
main = hspec spec
